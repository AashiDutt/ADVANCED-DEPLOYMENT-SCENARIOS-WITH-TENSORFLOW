{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "IMDB_REVIEWS_WITH_TF_HUB.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyNS1AtkKXnwBkrxaeepD/5x",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AashiDutt/ADVANCED-DEPLOYMENT-SCENARIOS-WITH-TENSORFLOW/blob/master/IMDB_REVIEWS_WITH_TF_HUB.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_szrcGVseTrS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "try:\n",
        "    %tensorflow_version 2.x\n",
        "except:\n",
        "    pass"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oyYRbgdyeULk",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "b1d0d1de-e7bd-4696-d975-abbb1efc6916"
      },
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow_hub as hub\n",
        "import tensorflow_datasets as tfds\n",
        "tfds.disable_progress_bar()\n",
        "\n",
        "print(\"\\u2022 Using TensorFlow Version:\", tf.__version__)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "• Using TensorFlow Version: 2.2.0-rc3\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tfIvJlJfeWgK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# DOWNLOAD IMDB DATASET\n",
        "splits = ['train[:60%]', 'train[-40%:]', 'test']\n",
        "\n",
        "splits, info = tfds.load(name=\"imdb_reviews\", with_info=True, split=splits, as_supervised=True)\n",
        "\n",
        "train_data, validation_data, test_data = splits"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TKaDa-uHedLg",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 84
        },
        "outputId": "6999c522-72c8-4e66-8971-1474c7834fb1"
      },
      "source": [
        "num_train_examples = info.splits['train'].num_examples\n",
        "num_test_examples = info.splits['test'].num_examples\n",
        "num_classes = info.features['label'].num_classes\n",
        "\n",
        "print('The Dataset has a total of:')\n",
        "print('\\u2022 {:,} classes'.format(num_classes))\n",
        "\n",
        "print('\\u2022 {:,} movie reviews for training'.format(num_train_examples))\n",
        "print('\\u2022 {:,} movie reviews for testing'.format(num_test_examples))"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The Dataset has a total of:\n",
            "• 2 classes\n",
            "• 25,000 movie reviews for training\n",
            "• 25,000 movie reviews for testing\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IAjdqCBsejc-",
        "colab_type": "text"
      },
      "source": [
        "The labels are either 0 or 1, where 0 is a negative review, and 1 is a positive review"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VFJuE4lhef6U",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class_names = ['negative', 'positive']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s4qUiOmVel_p",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 138
        },
        "outputId": "d87eb6fb-4d65-452a-8813-c4b45c7d2b77"
      },
      "source": [
        "for review, label in train_data.take(1):\n",
        "    review = review.numpy()\n",
        "    label = label.numpy()\n",
        "\n",
        "    print('\\nMovie Review:\\n\\n', review)\n",
        "    print('\\nLabel:', class_names[label])"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Movie Review:\n",
            "\n",
            " b\"This was an absolutely terrible movie. Don't be lured in by Christopher Walken or Michael Ironside. Both are great actors, but this must simply be their worst role in history. Even their great acting could not redeem this movie's ridiculous storyline. This movie is an early nineties US propaganda piece. The most pathetic scenes were those when the Columbian rebels were making their cases for revolutions. Maria Conchita Alonso appeared phony, and her pseudo-love affair with Walken was nothing but a pathetic emotional plug in a movie that was devoid of any real meaning. I am disappointed that there are movies like this, ruining actor's like Christopher Walken's good name. I could barely sit through it.\"\n",
            "\n",
            "Label: negative\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l3rFO5xdetGQ",
        "colab_type": "text"
      },
      "source": [
        "Load Word Embeddings\n",
        "\n",
        "In this example, the input data consists of sentences. The labels to predict are either 0 or 1.\n",
        "\n",
        "One way to represent the text is to convert sentences into word embeddings. Word embeddings, are an efficient way to represent words using dense vectors, where semantically similar words have similar vectors. We can use a pre-trained text embedding as the first layer of our model, which will have two advantages:\n",
        "\n",
        "We don't have to worry anout text preprocessing.\n",
        "We can benefit from transfer learning.\n",
        "For this example we will use a model from TensorFlow Hub called google/tf2-preview/gnews-swivel-20dim/1. We'll create a hub.KerasLayer that uses the TensorFlow Hub model to embed the sentences. We can choose to fine-tune the TF hub module weights during training by setting the trainable parameter to True."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1iIgLSGFeoF5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "embedding = \"https://tfhub.dev/google/tf2-preview/gnews-swivel-20dim/1\"\n",
        "\n",
        "hub_layer = hub.KerasLayer(embedding, input_shape=[], dtype=tf.string, trainable=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DrhLug2YexMx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# build pipeline\n",
        "batch_size = 512\n",
        "\n",
        "train_batches = train_data.shuffle(num_train_examples // 4).batch(batch_size).prefetch(1)\n",
        "validation_batches = validation_data.batch(batch_size).prefetch(1)\n",
        "test_batches = test_data.batch(batch_size)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wjo6vnIje3Fi",
        "colab_type": "text"
      },
      "source": [
        "Build the Model\n",
        "\n",
        "In the code below we will build a Keras Sequential model with the following layers:\n",
        "\n",
        "The first layer is a TensorFlow Hub layer. This layer uses a pre-trained SavedModel to map a sentence into its embedding vector. The model that we are using (google/tf2-preview/gnews-swivel-20dim/1) splits the sentence into tokens, embeds each token and then combines the embedding. The resulting dimensions are: (num_examples, embedding_dimension).\n",
        "This fixed-length output vector is piped through a fully-connected (Dense) layer with 16 hidden units.\n",
        "The last layer is densely connected with a single output node. Using the sigmoid activation function, this value is a float between 0 and 1, representing a probability, or confidence level."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XYWWdAcfe02O",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = tf.keras.Sequential([\n",
        "        hub_layer,\n",
        "        tf.keras.layers.Dense(16, activation='relu'),\n",
        "        tf.keras.layers.Dense(1, activation='sigmoid')])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "udxf8Z37e51_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 709
        },
        "outputId": "8eaffced-7b28-48a5-c72e-deab692898fd"
      },
      "source": [
        "# train the model\n",
        "model.compile(optimizer='adam',\n",
        "              loss='binary_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "history = model.fit(train_batches,\n",
        "                    epochs=20,\n",
        "                    validation_data=validation_batches)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n",
            "30/30 [==============================] - 3s 112ms/step - loss: 0.7652 - accuracy: 0.5485 - val_loss: 0.6670 - val_accuracy: 0.6038\n",
            "Epoch 2/20\n",
            "30/30 [==============================] - 3s 106ms/step - loss: 0.6332 - accuracy: 0.6428 - val_loss: 0.6143 - val_accuracy: 0.6654\n",
            "Epoch 3/20\n",
            "30/30 [==============================] - 3s 107ms/step - loss: 0.5938 - accuracy: 0.6875 - val_loss: 0.5820 - val_accuracy: 0.6988\n",
            "Epoch 4/20\n",
            "30/30 [==============================] - 3s 103ms/step - loss: 0.5591 - accuracy: 0.7217 - val_loss: 0.5504 - val_accuracy: 0.7311\n",
            "Epoch 5/20\n",
            "30/30 [==============================] - 3s 106ms/step - loss: 0.5229 - accuracy: 0.7561 - val_loss: 0.5208 - val_accuracy: 0.7577\n",
            "Epoch 6/20\n",
            "30/30 [==============================] - 3s 106ms/step - loss: 0.4887 - accuracy: 0.7793 - val_loss: 0.4926 - val_accuracy: 0.7799\n",
            "Epoch 7/20\n",
            "30/30 [==============================] - 3s 108ms/step - loss: 0.4571 - accuracy: 0.7996 - val_loss: 0.4657 - val_accuracy: 0.7946\n",
            "Epoch 8/20\n",
            "30/30 [==============================] - 3s 108ms/step - loss: 0.4230 - accuracy: 0.8207 - val_loss: 0.4365 - val_accuracy: 0.8085\n",
            "Epoch 9/20\n",
            "30/30 [==============================] - 3s 108ms/step - loss: 0.3840 - accuracy: 0.8397 - val_loss: 0.4076 - val_accuracy: 0.8231\n",
            "Epoch 10/20\n",
            "30/30 [==============================] - 3s 108ms/step - loss: 0.3488 - accuracy: 0.8575 - val_loss: 0.3829 - val_accuracy: 0.8374\n",
            "Epoch 11/20\n",
            "30/30 [==============================] - 3s 109ms/step - loss: 0.3191 - accuracy: 0.8747 - val_loss: 0.3631 - val_accuracy: 0.8471\n",
            "Epoch 12/20\n",
            "30/30 [==============================] - 3s 108ms/step - loss: 0.2942 - accuracy: 0.8865 - val_loss: 0.3478 - val_accuracy: 0.8525\n",
            "Epoch 13/20\n",
            "30/30 [==============================] - 3s 105ms/step - loss: 0.2714 - accuracy: 0.8978 - val_loss: 0.3354 - val_accuracy: 0.8597\n",
            "Epoch 14/20\n",
            "30/30 [==============================] - 3s 106ms/step - loss: 0.2508 - accuracy: 0.9079 - val_loss: 0.3254 - val_accuracy: 0.8638\n",
            "Epoch 15/20\n",
            "30/30 [==============================] - 3s 105ms/step - loss: 0.2328 - accuracy: 0.9153 - val_loss: 0.3181 - val_accuracy: 0.8685\n",
            "Epoch 16/20\n",
            "30/30 [==============================] - 3s 107ms/step - loss: 0.2173 - accuracy: 0.9219 - val_loss: 0.3129 - val_accuracy: 0.8694\n",
            "Epoch 17/20\n",
            "30/30 [==============================] - 3s 106ms/step - loss: 0.2034 - accuracy: 0.9296 - val_loss: 0.3072 - val_accuracy: 0.8731\n",
            "Epoch 18/20\n",
            "30/30 [==============================] - 3s 105ms/step - loss: 0.1905 - accuracy: 0.9353 - val_loss: 0.3053 - val_accuracy: 0.8727\n",
            "Epoch 19/20\n",
            "30/30 [==============================] - 3s 107ms/step - loss: 0.1796 - accuracy: 0.9391 - val_loss: 0.3049 - val_accuracy: 0.8723\n",
            "Epoch 20/20\n",
            "30/30 [==============================] - 3s 107ms/step - loss: 0.1669 - accuracy: 0.9445 - val_loss: 0.3015 - val_accuracy: 0.8751\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R5YBjW02e9TW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "69535611-9454-4d7a-a218-40dddb1fd45e"
      },
      "source": [
        "# evaluating model\n",
        "eval_results = model.evaluate(test_batches, verbose=0)\n",
        "\n",
        "for metric, value in zip(model.metrics_names, eval_results):\n",
        "    print(metric + ': {:.3}'.format(value))"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "loss: 0.313\n",
            "accuracy: 0.867\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}